{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb6147f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bJ40dxxPj0FN",
    "outputId": "5b78bdda-2040-41ce-aa46-fd3eb53d8563"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c59c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install soundfile audioread\n",
    "!pip install tqdm\n",
    "!pip install imageio[ffmpeg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aeca3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS=1000\n",
    "LR=0.01\n",
    "GAMMA=0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2c9f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!/usr/bin/python -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b92f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23844e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62193893",
   "metadata": {
    "id": "RXSo1xcTHC3n"
   },
   "source": [
    "IMPORTING REQUIRED LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eadde949",
   "metadata": {
    "id": "c8db6101"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "#import textgrid\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4e3ba81f",
   "metadata": {
    "id": "jw9RxwL7bc0C"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "38c2a86d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ed4cfd",
   "metadata": {
    "id": "5BtV--nzD__x"
   },
   "source": [
    "FOR CONVERTING TEXTGRID TO JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "96dd208d",
   "metadata": {
    "id": "3aa182b1"
   },
   "outputs": [],
   "source": [
    "# textgrid_dir = \"/content/drive/MyDrive/annotation\"\n",
    "# json_dir = \"/content/drive/MyDrive/json\"\n",
    "# audio_dir = \"/content/drive/MyDrive/Songs for annotation\"\n",
    "\n",
    "# if not os.path.exists(json_dir):\n",
    "#     os.makedirs(json_dir)\n",
    "\n",
    "# file_mapping = {}\n",
    "# counter = 1\n",
    "\n",
    "# for textgrid_file in os.listdir(textgrid_dir):\n",
    "#     textgrid_file_path = os.path.join(textgrid_dir, textgrid_file)\n",
    "\n",
    "#     try:\n",
    "#         tgrid = textgrid.read_textgrid(textgrid_file_path)\n",
    "\n",
    "\n",
    "#         new_filename = f\"{counter:03d}\"\n",
    "#         json_file_path = os.path.join(json_dir, f\"{new_filename}.json\")\n",
    "#         textgrid_new_file_path = os.path.join(textgrid_dir, f\"{new_filename}.TextGrid\")\n",
    "#         audio_new_file_path = os.path.join(audio_dir, f\"{new_filename}.mp3\")\n",
    "\n",
    "\n",
    "#         counter += 1\n",
    "\n",
    "\n",
    "#         file_mapping[textgrid_file] = new_filename\n",
    "\n",
    "\n",
    "#         pd.DataFrame(tgrid).to_json(json_file_path)\n",
    "\n",
    "\n",
    "#         os.rename(textgrid_file_path, textgrid_new_file_path)\n",
    "\n",
    "\n",
    "#         audio_original_file_path = os.path.join(audio_dir, textgrid_file.replace(\".TextGrid\", \".mp3\"))\n",
    "#         os.rename(audio_original_file_path, audio_new_file_path)\n",
    "\n",
    "#     except Exception as e:\n",
    "#         pass\n",
    "\n",
    "# # Save the mapping to a CSV file for reference\n",
    "# mapping_df = pd.DataFrame(list(file_mapping.items()), columns=['OriginalFilename', 'NewFilename'])\n",
    "# mapping_df.to_csv(\"/content/drive/MyDrive/filename_mapping.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d515fd35",
   "metadata": {
    "id": "PnJM7eYlHMq3"
   },
   "source": [
    "DATASET CLASS AND DATALOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "143a6870",
   "metadata": {
    "id": "_sdTOhbLWgMc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pydub import AudioSegment\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "class MusicDataset(Dataset):\n",
    "    def __init__(self, audio_dir, json_dir):\n",
    "        self.audio_dir = audio_dir\n",
    "        self.json_dir = json_dir\n",
    "        self.audio_files = [file for file in os.listdir(audio_dir) if file.endswith(\".mp3\")]\n",
    "        self.json_files = [file for file in os.listdir(json_dir) if file.endswith(\".json\")]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_file_path = os.path.join(self.audio_dir, self.audio_files[idx])\n",
    "        \n",
    "        # Use pydub for audio loading\n",
    "        audio = AudioSegment.from_mp3(audio_file_path)\n",
    "        y = np.array(audio.get_array_of_samples(),dtype=float)\n",
    "        sr = audio.frame_rate\n",
    "\n",
    "        json_file_path = os.path.join(self.json_dir, self.audio_files[idx].replace(\".mp3\", \".json\"))\n",
    "        with open(json_file_path, \"r\") as f:\n",
    "            json_data = json.load(f)\n",
    "\n",
    "        # Extract start and stop timings from the JSON data\n",
    "        start_times = [json_data[\"start\"][str(i)] for i in range(len(json_data[\"start\"]))]\n",
    "        stop_times = [json_data[\"stop\"][str(i)] for i in range(len(json_data[\"stop\"]))]\n",
    "\n",
    "        # Pad start and stop times\n",
    "        start_times = torch.nn.functional.pad(torch.FloatTensor(start_times), (0, 15 - len(start_times)))\n",
    "        stop_times = torch.nn.functional.pad(torch.FloatTensor(stop_times), (0, 15 - len(stop_times)))\n",
    "\n",
    "        # Extract MFCCs using librosa\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "        mfcc = torch.nn.functional.pad(torch.FloatTensor(mfcc), (0, 100 - mfcc.shape[1]))\n",
    "\n",
    "        # Extract Chroma using librosa\n",
    "        chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "        chroma = torch.nn.functional.pad(torch.FloatTensor(chroma), (0, 100 - chroma.shape[1]))\n",
    "\n",
    "        # Extract Tempo using librosa\n",
    "        onset_env = librosa.onset.onset_strength(y=y, sr=sr)\n",
    "        tempo, _ = librosa.beat.beat_track(onset_envelope=onset_env, sr=sr)\n",
    "        # Extract Rhythm Patterns (Example: using tempogram) using librosa\n",
    "        tempogram = librosa.feature.tempogram(onset_envelope=onset_env, sr=sr)\n",
    "        rhythm_patterns = torch.nn.functional.pad(torch.FloatTensor(tempogram), (0, 100 - tempogram.shape[1]))\n",
    "\n",
    "        sample = {\n",
    "            'start_times': start_times,\n",
    "            'stop_times': stop_times,\n",
    "            'mfcc': mfcc,\n",
    "            'chroma': chroma,\n",
    "            'tempo': torch.FloatTensor([tempo]),  # Tempo as a single value\n",
    "            'rhythm_patterns': rhythm_patterns\n",
    "        }\n",
    "\n",
    "        return sample\n",
    "\n",
    "# Set your directory paths\n",
    "audio_directory = \"./songs\"\n",
    "json_directory = \"./json\"\n",
    "\n",
    "# Create dataset and dataloader\n",
    "music_dataset = MusicDataset(audio_directory, json_directory)\n",
    "music_dataloader = DataLoader(music_dataset, batch_size=50, shuffle=True, collate_fn=lambda x: x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5a51b781",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loader = DataLoader(music_dataset, batch_size=1, shuffle=True, collate_fn=lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "371c5962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'start_times': tensor([  0.0000,  25.1465,  97.1913, 121.1918, 157.2372, 169.3034, 196.1156,\n",
      "        232.2465, 282.2249,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000]), 'stop_times': tensor([ 25.1465,  97.1913, 121.1918, 157.2372, 169.3034, 196.1156, 232.2465,\n",
      "        282.2249, 288.2732,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "          0.0000]), 'mfcc': tensor([[504.4515, 504.4515, 504.4515,  ..., 504.4515, 504.4515, 504.4515],\n",
      "        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "        ...,\n",
      "        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,  ...,   0.0000,   0.0000,   0.0000]]), 'chroma': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]), 'tempo': tensor([120.1853]), 'rhythm_patterns': tensor([[ 1.0000e+00,  1.0000e+00,  1.0000e+00,  ...,  1.0000e+00,\n",
      "          1.0000e+00,  1.0000e+00],\n",
      "        [ 4.9195e-01,  5.0083e-01,  5.1150e-01,  ...,  9.2907e-01,\n",
      "          9.2883e-01,  9.2859e-01],\n",
      "        [ 3.0236e-01,  3.1239e-01,  3.2478e-01,  ...,  8.7556e-01,\n",
      "          8.7520e-01,  8.7484e-01],\n",
      "        ...,\n",
      "        [-1.1929e-17, -2.2573e-18, -2.2730e-17,  ...,  6.1559e-17,\n",
      "          1.9378e-17, -2.7392e-17],\n",
      "        [ 7.4850e-18, -1.6311e-17, -2.2244e-17,  ...,  5.5405e-17,\n",
      "          4.7067e-18,  2.3475e-19],\n",
      "        [-6.4714e-18,  3.9569e-18, -2.4391e-17,  ...,  6.5811e-17,\n",
      "         -1.1755e-17, -2.7912e-17]])}]\n"
     ]
    }
   ],
   "source": [
    "for i in valid_loader:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce65fe6",
   "metadata": {
    "id": "HCCa0vbZHTLp"
   },
   "source": [
    "MODEL CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ad9ebc4e",
   "metadata": {
    "id": "WxdEDJEkKADK"
   },
   "outputs": [],
   "source": [
    "# from transformers import HubertModel, HubertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8c1f4698",
   "metadata": {
    "id": "CbuapN-AKN96"
   },
   "outputs": [],
   "source": [
    "class AudioModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AudioModel, self).__init__()\n",
    "\n",
    "        # Load pre-trained Hubert model\n",
    "        # config = HubertConfig.from_pretrained(\"facebook/hubert-large-ll60k\")\n",
    "        # self.audio_encoder = HubertModel.from_pretrained(\"facebook/hubert-large-ll60k\", config=config)\n",
    "\n",
    "        self.feature_encoder = nn.Sequential(\n",
    "            nn.Linear(in_features=100, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "\n",
    "        self.conv1 = nn.Conv1d(128, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv1d(64, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv1d(32, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv1d(16, 1, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.fc_start = nn.Linear(in_features=409, out_features=15)  # Adjusted output size to match ground truth\n",
    "        self.fc_stop = nn.Linear(in_features=409, out_features=15)   # Adjusted output size to match ground truth\n",
    "\n",
    "    def forward(self, mfcc, chroma, rhythm_patterns):\n",
    "        # Combine audio features with other features\n",
    "        combined_features = torch.cat([mfcc, chroma, rhythm_patterns], dim=1)\n",
    "\n",
    "        # Encode combined features\n",
    "        features_encoded = self.feature_encoder(combined_features)\n",
    "\n",
    "        # Transpose to fit the expected shape for 1D convolution\n",
    "        features_encoded = features_encoded.permute(0, 2, 1)\n",
    "\n",
    "        # Apply 1D convolutions\n",
    "        conv1_output = self.conv1(features_encoded)\n",
    "        conv2_output = self.conv2(conv1_output)\n",
    "        conv3_output = self.conv3(conv2_output)\n",
    "        conv4_output = self.conv4(conv3_output)\n",
    "\n",
    "        # Squeeze to remove the extra dimension added by the convolution\n",
    "        conv_output = conv4_output.squeeze(dim=2)\n",
    "\n",
    "        # Predict start and stop times\n",
    "        start_times = self.fc_start(conv_output)\n",
    "        stop_times = self.fc_stop(conv_output)\n",
    "\n",
    "        return start_times, stop_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9aae3cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ExponentialLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0ab80441",
   "metadata": {
    "id": "39vK5U4PKbST"
   },
   "outputs": [],
   "source": [
    "model = AudioModel()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = ExponentialLR(optimizer, gamma=GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "36ff4a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss_array=[]\n",
    "valid_loss_array=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "eb8cf537",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r05gPRe9bq2w",
    "outputId": "fa08cfa2-80b5-43b8-f1aa-698c078178ac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AudioModel(\n",
       "  (feature_encoder): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (conv1): Conv1d(128, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (conv2): Conv1d(64, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (conv3): Conv1d(32, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (conv4): Conv1d(16, 1, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (fc_start): Linear(in_features=409, out_features=15, bias=True)\n",
       "  (fc_stop): Linear(in_features=409, out_features=15, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fdcc38",
   "metadata": {
    "id": "F4k4r09wHRdb"
   },
   "source": [
    "TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08eed8c3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dKCLhPBbFx6e",
    "outputId": "49d4d7a3-a261-4c85-9273-22d81f2bd9a8"
   },
   "outputs": [],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    loss=0.0\n",
    "    total_loss=0.0\n",
    "    for batch in tqdm(music_dataloader):\n",
    "        # audio_data = pad_sequence([sample['audio'] for sample in batch], batch_first=True)\n",
    "        mfcc = pad_sequence([sample['mfcc'] for sample in batch], batch_first=True)\n",
    "        chroma = pad_sequence([sample['chroma'] for sample in batch], batch_first=True)\n",
    "        rhythm_patterns = pad_sequence([sample['rhythm_patterns'] for sample in batch], batch_first=True)\n",
    "        start_times = pad_sequence([sample['start_times'] for sample in batch], batch_first=True)\n",
    "        stop_times = pad_sequence([sample['stop_times'] for sample in batch], batch_first=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        start_pred, stop_pred = model(mfcc.to(device), chroma.to(device), rhythm_patterns.to(device))\n",
    "        start_pred = start_pred.squeeze(dim=1)\n",
    "        stop_pred = stop_pred.squeeze(dim=1)\n",
    "        loss_start = criterion(start_pred.to(device), start_times.to(device))\n",
    "        loss_stop = criterion(stop_pred.to(device), stop_times.to(device))\n",
    "\n",
    "        total_loss = loss_start + loss_stop\n",
    "        loss += total_loss.item()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    avg_training_loss = loss / len(music_dataloader)\n",
    "    training_loss_array.append(avg_training_loss)\n",
    "    model.eval()  \n",
    "    total_valid_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation during validation\n",
    "        for batch in tqdm(valid_loader):\n",
    "            mfcc = pad_sequence([sample['mfcc'] for sample in batch], batch_first=True)\n",
    "            chroma = pad_sequence([sample['chroma'] for sample in batch], batch_first=True)\n",
    "            rhythm_patterns = pad_sequence([sample['rhythm_patterns'] for sample in batch], batch_first=True)\n",
    "            start_times = pad_sequence([sample['start_times'] for sample in batch], batch_first=True)\n",
    "            stop_times = pad_sequence([sample['stop_times'] for sample in batch], batch_first=True)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            start_pred, stop_pred = model(mfcc.to(device), chroma.to(device), rhythm_patterns.to(device))\n",
    "            start_pred = start_pred.squeeze(dim=1)\n",
    "            stop_pred = stop_pred.squeeze(dim=1)\n",
    "            loss_start = criterion(start_pred.to(device), start_times.to(device))\n",
    "            loss_stop = criterion(stop_pred.to(device), stop_times.to(device))\n",
    "\n",
    "            total_valid_loss += (loss_start + loss_stop).item()\n",
    "\n",
    "    avg_valid_loss = total_valid_loss / len(valid_loader)\n",
    "    valid_loss_array.append(avg_valid_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Training Loss: {avg_training_loss} | Validation Loss: {avg_valid_loss}\")\n",
    "    if epoch%25 == 0:\n",
    "        torch.save(model,f\"Checkpoint:Epoch-{epoch+1}.pth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c3ba35",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = [i for i in range(1, 54)]  # Epochs from 1 to 53\n",
    "\n",
    "training_loss = [\n",
    "    45991.265625, 44109.0703125, 33472.6953125, 33352.515625, 25990.76171875,\n",
    "    24380.203125, 23231.09375, 22515.69140625, 21980.501953125, 21303.6640625,\n",
    "    20855.671875, 20493.640625, 19928.1015625, 19772.16015625, 19622.189453125,\n",
    "    19425.78515625, 19359.923828125, 18570.5, 19108.708984375, 19121.466796875,\n",
    "    19270.287109375, 19576.7890625, 19225.01953125, 19228.751953125,\n",
    "    19706.8515625, 19164.138671875, 19812.93359375, 18696.76171875, 19321.0625,\n",
    "    19023.404296875, 19191.533203125, 19239.54296875, 19265.876953125, 19534.625,\n",
    "    19259.009765625, 19096.033203125, 19422.9921875, 19276.90234375,\n",
    "    19080.728515625, 19549.556640625, 19505.57421875, 18923.021484375,\n",
    "    19383.07421875, 19426.2890625, 18953.04296875, 19432.23046875, 19282.578125,\n",
    "    19707.91015625, 19424.439453125, 19080.923828125, 19880.771484375,\n",
    "    18925.181640625, 19818.59375\n",
    "]\n",
    "\n",
    "validation_loss = [\n",
    "    44288.341796875, 33471.9619140625, 32385.208984375, 25410.986328125,\n",
    "    23953.521484375, 23123.837890625, 22390.798828125, 21395.77734375,\n",
    "    20538.5146484375, 19858.7685546875, 19689.447265625, 19281.96484375,\n",
    "    19350.666015625, 19089.0009765625, 19017.884765625, 19032.9375,\n",
    "    18867.1533203125, 18875.02734375, 18728.6962890625, 18876.05078125,\n",
    "    18645.6845703125, 18779.103515625, 18853.35546875, 18963.5146484375,\n",
    "    18714.0927734375, 18692.8818359375, 18796.2060546875, 18721.640625,\n",
    "    18668.7373046875, 18957.138671875, 18937.251953125, 18729.8037109375,\n",
    "    18745.0078125, 18862.470703125, 18699.2861328125, 18644.65625,\n",
    "    18840.8427734375, 18597.466796875, 18785.1865234375, 18760.232421875,\n",
    "    18637.6591796875, 18544.8935546875, 18701.7451171875, 18799.20703125,\n",
    "    18857.5146484375, 18658.4208984375, 18744.328125, 18797.5478515625,\n",
    "    18590.28125, 18825.9189453125, 18536.0166015625, 18749.1279296875,\n",
    "    18763.5927734375\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c208e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, training_loss, label='Training Loss', marker='o')\n",
    "plt.plot(epochs, validation_loss, label='Validation Loss', marker='o')\n",
    "plt.title('Training and Validation Loss over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('res.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab101e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e89364",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=torch.load('./Checkpoint:Epoch-51.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5cb07fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6bc13f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[  9.8808,   4.4377,  79.4566,  88.4239,  67.0505, 122.4196, 158.4926,\n",
      "           64.3213, 167.2208, 124.7245,  53.0197,  72.3904,  75.0917,  25.4285,\n",
      "            4.0510]]], device='cuda:0', grad_fn=<ViewBackward0>), tensor([[[ 19.8340,  85.4359,  89.0912,  74.8544,  61.1949, 159.1323, 185.5056,\n",
      "          173.6344, 163.6078, -41.2310,  90.8854,  66.6665,  32.1955,  34.3917,\n",
      "           36.9823]]], device='cuda:0', grad_fn=<ViewBackward0>))\n",
      "ACTUAL OUTPUT:\n",
      "tensor([[  0.0000,  20.6574,  63.2355,  85.6708, 135.2309, 156.3171, 191.6839,\n",
      "         241.4335, 283.9351,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000]]) tensor([[ 20.6574,  63.2355,  85.6708, 135.2309, 156.3171, 191.6839, 241.4335,\n",
      "         283.9351, 290.2846,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000]])\n"
     ]
    }
   ],
   "source": [
    "for batch in valid_loader:\n",
    "    mfcc = pad_sequence([sample['mfcc'] for sample in batch], batch_first=True)\n",
    "    chroma = pad_sequence([sample['chroma'] for sample in batch], batch_first=True)\n",
    "    rhythm_patterns = pad_sequence([sample['rhythm_patterns'] for sample in batch], batch_first=True)\n",
    "    start_times = pad_sequence([sample['start_times'] for sample in batch], batch_first=True)\n",
    "    stop_times = pad_sequence([sample['stop_times'] for sample in batch], batch_first=True)\n",
    "\n",
    "    print(model(mfcc.to(device), chroma.to(device), rhythm_patterns.to(device)))\n",
    "    \n",
    "    print(\"ACTUAL OUTPUT:\")\n",
    "    \n",
    "    print(start_times,stop_times)\n",
    "    \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e17d633",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig(\"results.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0108356",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6526d6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
