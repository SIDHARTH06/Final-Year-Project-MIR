{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8db6101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import textgrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c54089a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper Parameters\n",
    "SAMPLING_RATE = 22050\n",
    "NO_MFCC = 13\n",
    "HOP_LENGTH = 512\n",
    "DURATION = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa182b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import textgrid\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    textgrid_dir = \"/path/to/textgrid/dir\"\n",
    "    json_dir = \"/path/to/json/dir\"\n",
    "\n",
    "    # Create the json directory if it doesn't exist.\n",
    "    if not os.path.exists(json_dir):\n",
    "        os.makedirs(json_dir)\n",
    "\n",
    "    # Iterate over all textgrid files in the textgrid directory and convert them to json files.\n",
    "    for textgrid_file in os.listdir(textgrid_dir):\n",
    "        textgrid_file_path = os.path.join(textgrid_dir, textgrid_file)\n",
    "        json_file_path = os.path.join(json_dir, os.path.splitext(textgrid_file)[0] + \".json\")\n",
    "\n",
    "        convert_textgrid_to_json(textgrid_file_path, json_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c072cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicSegmentationDataset(Dataset):\n",
    "    def __init__(self, root_dir, sr=SAMPLING_RATE, n_mfcc=NO_MFCC, hop_length=HOP_LENGTH, duration=DURATION):\n",
    "        self.root_dir = root_dir\n",
    "        self.song_files = os.listdir(os.path.join(self.root_dir, 'songs'))\n",
    "        self.metadata_dir = os.path.join(self.root_dir, 'metadata')\n",
    "        self.sr = sr\n",
    "        self.n_mfcc = n_mfcc\n",
    "        self.hop_length = hop_length\n",
    "        self.duration = duration\n",
    "\n",
    "    def convert_textgrid_to_json(self,textgrid_file, json_file):\n",
    "        textgrid_object = textgrid.TextGrid.load(textgrid_file)\n",
    "\n",
    "        json_object = {\n",
    "            \"tiers\": []\n",
    "        }\n",
    "\n",
    "        for tier in textgrid_object.tiers:\n",
    "            json_tier = {\n",
    "                \"name\": tier.name,\n",
    "                \"intervals\": []\n",
    "            }\n",
    "\n",
    "            for interval in tier.intervals:\n",
    "                json_interval = {\n",
    "                    \"xmin\": interval.xmin,\n",
    "                    \"xmax\": interval.xmax,\n",
    "                    \"text\": interval.text\n",
    "                }\n",
    "\n",
    "                json_tier[\"intervals\"].append(json_interval)\n",
    "\n",
    "            json_object[\"tiers\"].append(json_tier)\n",
    "\n",
    "        with open(json_file, \"w\") as f:\n",
    "            json.dump(json_object, f, indent=4)\n",
    "\n",
    "    def _load_ground_truth(self, song_name):\n",
    "        metadata_file = os.path.join(self.metadata_dir, f\"{song_name}.json\")\n",
    "        if os.path.exists(metadata_file):\n",
    "            with open(metadata_file, 'r') as json_file:\n",
    "                metadata = json.load(json_file)\n",
    "            return {\n",
    "                'pallavi': torch.tensor(metadata['pallavi']),\n",
    "                'anupallavi': torch.tensor(metadata['anupallavi']),\n",
    "                'charanam': torch.tensor(metadata['charanam']),\n",
    "                'bgm': torch.tensor(metadata['bgm'])\n",
    "            }\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _extract_features(self, music_file):\n",
    "        y, sr = librosa.load(music_file, sr=self.sr, duration=self.duration)\n",
    "        \n",
    "        # Extract MFCCs\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=self.n_mfcc, hop_length=self.hop_length)\n",
    "        \n",
    "        # Extract chroma features\n",
    "        chroma = librosa.feature.chroma_stft(y=y, sr=sr, hop_length=self.hop_length)\n",
    "        \n",
    "        # Extract mel-spectrogram features\n",
    "        mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, hop_length=self.hop_length)\n",
    "        \n",
    "        return torch.tensor(mfcc.T, dtype=torch.float32), \\\n",
    "               torch.tensor(chroma.T, dtype=torch.float32), \\\n",
    "               torch.tensor(mel_spec.T, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.song_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        song_name = self.song_files[idx]\n",
    "        music_file = os.path.join(self.root_dir, 'songs', song_name)\n",
    "        ground_truth = self._load_ground_truth(song_name)\n",
    "\n",
    "        if ground_truth is not None:\n",
    "            mfcc, chroma, mel_spec = self._extract_features(music_file)\n",
    "            return {\n",
    "                'mfcc': mfcc,\n",
    "                'chroma': chroma,\n",
    "                'mel_spec': mel_spec,\n",
    "                'ground_truth': ground_truth\n",
    "            }\n",
    "        else:\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69aa3fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MusicSegmentationDataset(root_dir=os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5dc2c280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/Desktop/Final/Final-Year-Project-MIR/metadata/27_Anbendra Mazhaiyilae - Minsara Kanavu  Anuradha Sriram  A.R. Rahman.mp3.json\n"
     ]
    }
   ],
   "source": [
    "dataset._load_ground_truth(dataset.song_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed5a721",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class MusicSegmentationModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(MusicSegmentationModel, self).__init__()\n",
    "        \n",
    "        # CNN layers for feature extraction\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=(3, 3), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2)),\n",
    "            nn.Conv2d(32, 64, kernel_size=(3, 3), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2)),\n",
    "            nn.Conv2d(64, 128, kernel_size=(3, 3), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2))\n",
    "        )\n",
    "        \n",
    "        # LSTM layers for sequence modeling\n",
    "        self.lstm = nn.LSTM(input_size=128, hidden_size=hidden_size, num_layers=2, batch_first=True, dropout=0.2)\n",
    "        \n",
    "        # Fully connected layer for classification\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Apply CNN to input features\n",
    "        x = self.cnn(x)\n",
    "        \n",
    "        # Reshape for LSTM\n",
    "        x = x.view(x.size(0), x.size(1), -1)\n",
    "        \n",
    "        # Apply LSTM\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # Get the output at each timestamp\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Apply fully connected layer for classification\n",
    "        output = self.fc(lstm_out)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Example usage:\n",
    "input_size = 128  # Adjust this based on your input feature size\n",
    "hidden_size = 64  # Adjust this based on your desired hidden size\n",
    "num_classes = 4   # Number of classes: pallavi, anupallavi, charanam, bgm\n",
    "\n",
    "model = MusicSegmentationModel(input_size, hidden_size, num_classes)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Data preprocessing (assuming you have a dataset with input features and labels)\n",
    "# Your labels may look something like this (timestamps for each segment type):\n",
    "labels = {\n",
    "    'pallavi': [(start1, end1), (start2, end2), ...],\n",
    "    'anupallavi': [(start1, end1), (start2, end2), ...],\n",
    "    'charanam': [(start1, end1), (start2, end2), ...],\n",
    "    'bgm': [(start1, end1), (start2, end2), ...]\n",
    "}\n",
    "\n",
    "# You need to convert the labels to a format suitable for training (e.g., one-hot encoding or integer labels).\n",
    "\n",
    "# Training loop (assuming you have input data as `input_data`)\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, segment_labels in zip(input_data, labels):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Convert segment_labels to a suitable format (e.g., integer labels)\n",
    "        # Apply appropriate loss function based on your label format\n",
    "        \n",
    "        loss = criterion(outputs, segment_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
